{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myenv(3.12.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rectified_text_2023_2.txt<br>\n",
    "RecursiveCharacterTextSplitter<br>\n",
    "jeonseonjin/embedding_BAAI-bge-m3<br>\n",
    "FAISS<br>\n",
    "Retrieval 과정에서 중복제거<br>\n",
    "gpt-4o<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install huggingface_hub langchain langchain-community faiss-cpu sentence-transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use 3.12.4\n",
    "import openai\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "key_path = '/Users/jaesolshin/key/openai_key.txt'\n",
    "os.environ[\"OPENAI_API_KEY\"] = open(key_path, 'r', encoding='utf-8').read()\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/Users/jaesolshin/Documents/GitHub/bokbot/corrected_text'\n",
    "documents = []\n",
    "\n",
    "# 폴더 내 파일을 하나씩 처리\n",
    "for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if os.path.isfile(file_path) and file_name.endswith('.txt'):\n",
    "        loader = TextLoader(file_path)\n",
    "        documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할 전 문서 개수: 20\n",
      "분할 후 문서 개수: 13078\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,   # 원하는 청크 크기 설정\n",
    "    chunk_overlap=50,  # 중첩되는 토큰 수\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \"]  # 사용자 정의 분할 함수 추가\n",
    ")\n",
    "\n",
    "#text_splitter = CharacterTextSplitter(separator = \"\\n\", chunk_size = 300, chunk_overlap  = 100, length_function = len,)\n",
    "\n",
    "splitted_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# 분할 후 각 문서의 내용을 정리\n",
    "# 문장이 마침표로 시작하면 마침표 제거\n",
    "# 문장이 마침표로 끝나지 않으면 마침표 추가\n",
    "for i in range(len(splitted_documents)):\n",
    "    splitted_documents[i].page_content = splitted_documents[i].page_content.strip('. ').strip()\n",
    "    if not splitted_documents[i].page_content.endswith('.'):\n",
    "        splitted_documents[i].page_content += '.'\n",
    "\n",
    "# 분할 결과 출력\n",
    "print(f'분할 전 문서 개수: {len(documents)}')\n",
    "print(f'분할 후 문서 개수: {len(splitted_documents)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/var/folders/hw/9m3g7fvn4_l3rp2y473km9sm0000gn/T/ipykernel_5130/4183026033.py:15: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"jeonseonjin/embedding_BAAI-bge-m3\")\n",
      "/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 배치 단위로 임베딩 처리\u001b[39;00m\n\u001b[1;32m     18\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtexts\u001b[49m), batch_size):\n\u001b[1;32m     20\u001b[0m     batch_texts \u001b[38;5;241m=\u001b[39m texts[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m     21\u001b[0m     batch_embeddings \u001b[38;5;241m=\u001b[39m embedding_model\u001b[38;5;241m.\u001b[39membed_documents(batch_texts)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "#분할된 문장을 벡터db에 저장\n",
    "#!pip install tiktoken\n",
    "#!pip install chromadb==0.5.3 \n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.docstore.document import Document\n",
    "from langchain import FAISS\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 배치 크기 설정\n",
    "batch_size = 1  # 사용할 수 있는 메모리에 맞는 배치 크기를 설정\n",
    "\n",
    "# SentenceTransformer 모델 로드\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"jeonseonjin/embedding_BAAI-bge-m3\")\n",
    "\n",
    "# 배치 단위로 임베딩 처리\n",
    "embeddings = []\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i + batch_size]\n",
    "    batch_embeddings = embedding_model.embed_documents(batch_texts)\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "d = len(embeddings[0])  # 임베딩 차원 (예: 384차원)\n",
    "index = faiss.IndexFlatIP(d)  # 내적을 사용하여 코사인 유사도 기반 인덱스 생성\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# FAISS 인덱스를 파일로 저장\n",
    "faiss.write_index(index, 'faiss_index.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index loaded from file.\n",
      "문서 개수: 5\n",
      "문서 내용: 한국은행은 이러한 정책 목표를 효과적으로 달성하는 방향으로 기준금리를 운영하고 있으며, 공개시장 운영, 여수신 제도, 지급준비 제도 등의 정책 수단을 활용하고 있다.\n",
      "문서 내용: 통화신용정책 한국은행은 통화신용정책의 효율적인 수립과 집행을 통해 물가 안정을 도모하고 금융 안정에도 유의하면서 국민경제의 건전한 발전에 이바지하기 위해 최선의 노력을 기울이고 있다. 통화정책 운영체계로 물가 안정 목표제를 채택하고 있으며 동 체계하에서도 정책 결정은 물가 상황 외에 경제 성장, 금융 안정 등을 종합적으로 고려하여 이루어진다.\n",
      "문서 내용: 한국은행이 수행한 통화신용정책의 상세한 내용은 한국은행이 발간하는 ｢통화신용정책보고서(분기)｣의 ｢Ⅱ. 통화신용정책 운영｣에서 확인할 수 있다. 한국은행의 통화정책 체계에 대한 상세한 설명은 한국은행이 발간한 ｢한국의 통화정책(2017)｣에서 확인할 수 있다.업무 현황 통화신용정책이라고 판단하였기 때문이다.\n",
      "문서 내용: 가. 물가 안정 목표 한국은행은 2016년 이후 물가 안정 목표를 소비자 물가 상승률(전년 동기 대비) 기준 2%로 설정하고 있다. 2019년부터 적용되는 ｢2019년 이후 물가 안정 목표｣ 설정 시에는 목표 수준을 종전(2016∼18년)과 동일하게 2%로 유지하는 가운데, 물가 안정 목표의 적용 기간을 특정하지 않는 방식으로 변경하였다.\n",
      "문서 내용: 한국은행은 이러한 대내외 정책 여건, 즉 물가 상승률이 기조적인 둔화 흐름을 이어갔지만 연중 물가 목표를 상회하는 오름세가 이어지고 경기와 금융안정 측면의 리스크가 상존하고 있었던 데다가 가계부채 추이, 미연준 통화정책, 지정학적 리스크 등과 관련한 높은 불확실성도 지속된 점을 고려할 때 기준금리를 긴축적인 수준에서 유지하고 그간의 3.0% 포인트 인상의 파급 효과를 점검하면서 추가 인상 필요성을 판단해 나가는 것이 필요하다.주요 이슈이다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 벡터db에 연결\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.docstore.document import Document\n",
    "from langchain import FAISS\n",
    "\n",
    "# FAISS 인덱스를 파일에서 불러오기\n",
    "if os.path.exists('faiss_index.bin'):\n",
    "    index = faiss.read_index('faiss_index.bin')\n",
    "    print(\"FAISS index loaded from file.\")\n",
    "else:\n",
    "    print(\"FAISS index file not found.\")\n",
    "\n",
    "# SentenceTransformer 모델 로드\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"jeonseonjin/embedding_BAAI-bge-m3\")\n",
    "\n",
    "# LangChain의 FAISS와 연결\n",
    "docstore = InMemoryDocstore({i: Document(page_content=texts[i]) for i in range(len(texts))})\n",
    "database = FAISS(embedding_function=embedding_model, index=index, docstore=docstore, index_to_docstore_id={i: i for i in range(len(texts))})\n",
    "\n",
    "# 쿼리 제시\n",
    "query = '2023년 한국은행의 정책목표 중 가장 중요한 것은 무엇인가? 답변과 함께 근거를 제시하라'\n",
    "results = database.similarity_search(query, k=5) \n",
    "\n",
    "\n",
    "# 답변에서 중복 제거\n",
    "unique_results = {result.page_content: result for result in results}.values()\n",
    "\n",
    "print(f'문서 개수: {len(unique_results)}')\n",
    "\n",
    "for d in unique_results:\n",
    "    print(f'문서 내용: {d.page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 개수: 5\n",
      "문서 내용: 한국은행은 이러한 정책 목표를 효과적으로 달성하는 방향으로 기준금리를 운영하고 있으며, 공개시장 운영, 여수신 제도, 지급준비 제도 등의 정책 수단을 활용하고 있다.\n",
      "문서 내용: 한국은행은 이러한 대내외 정책 여건, 즉 물가 상승률이 기조적인 둔화 흐름을 이어갔지만 연중 물가 목표를 상회하는 오름세가 이어지고 경기와 금융안정 측면의 리스크가 상존하고 있었던 데다가 가계부채 추이, 미연준 통화정책, 지정학적 리스크 등과 관련한 높은 불확실성도 지속된 점을 고려할 때 기준금리를 긴축적인 수준에서 유지하고 그간의 3.0% 포인트 인상의 파급 효과를 점검하면서 추가 인상 필요성을 판단해 나가는 것이 필요하다.주요 이슈이다.\n",
      "문서 내용: 한국은행은 콜금리(익일물)를 금융통화위원회가 정한 기준금리 수준에서 유지하기 위해 통화안정증권, 환매조건부증권(RP), 통화안정계정 등 다양한 공개시장 운영 수단을 활용하여 유동성을 신축적으로 조절하였다. 아울러 공개시장 운영 여건 변화에 맞춰 통화안정증권 발행 제도를 개선함으로써 유동성 조절의 효율성을 제고하였다.\n",
      "문서 내용: 기준금리를 3.5%의 긴축적인 수준으로 유지하고, 새마을금고 예금인출사태 등 금융·외환시장 불안에는 시장안정화 조치를 통해 적극 대처하였다. 한국은행은 경제상황에 대한 정확한 진단과 정교한 정책대응을 뒷받침하고자 조사 및 정책연구업무를 적극적으로 수행하였다.\n",
      "문서 내용: 나. 기준금리 한국은행은 2023년 중 성장세를 점검하면서 중기적 시계에서 물가 상승률이 목표 수준(2%)에서 안정될 수 있도록 기준금리를 긴축적인 수준에서 운용하였다.\n"
     ]
    }
   ],
   "source": [
    "query = '최근 한국은행이 기준금리를 조정한 이유는 무엇인가요?'\n",
    "\n",
    "results = database.similarity_search(query, k=5) \n",
    "\n",
    "# 답변에서 중복 제거\n",
    "unique_results = {result.page_content: result for result in results}.values()\n",
    "\n",
    "print(f'문서 개수: {len(unique_results)}')\n",
    "\n",
    "for d in unique_results:\n",
    "    print(f'문서 내용: {d.page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 개수: 5\n",
      "문서 내용: 통화신용정책 한국은행은 통화신용정책의 효율적인 수립과 집행을 통해 물가 안정을 도모하고 금융 안정에도 유의하면서 국민경제의 건전한 발전에 이바지하기 위해 최선의 노력을 기울이고 있다. 통화정책 운영체계로 물가 안정 목표제를 채택하고 있으며 동 체계하에서도 정책 결정은 물가 상황 외에 경제 성장, 금융 안정 등을 종합적으로 고려하여 이루어진다.\n",
      "문서 내용: 한국은행이 수행한 통화신용정책의 상세한 내용은 한국은행이 발간하는 ｢통화신용정책보고서(분기)｣의 ｢Ⅱ. 통화신용정책 운영｣에서 확인할 수 있다. 한국은행의 통화정책 체계에 대한 상세한 설명은 한국은행이 발간한 ｢한국의 통화정책(2017)｣에서 확인할 수 있다.업무 현황 통화신용정책이라고 판단하였기 때문이다.\n",
      "문서 내용: 한국은행은 통화신용정책 등을 수행함에 있어 금융기관 등을 대상으로 여수신 업무를 수행한다. 2023년 중 운용한 여신 업무로는 금융중개지원대출, 자금조정대출, 일중당좌대출 등이 있으며, 수신 업무로는 당좌예금, 자금조정예금 및 통화 안정계정 예치금 등이 있다.\n",
      "문서 내용: 한국은행은 콜금리(익일물)를 금융통화위원회가 정한 기준금리 수준에서 유지하기 위해 통화안정증권, 환매조건부증권(RP), 통화안정계정 등 다양한 공개시장 운영 수단을 활용하여 유동성을 신축적으로 조절하였다. 아울러 공개시장 운영 여건 변화에 맞춰 통화안정증권 발행 제도를 개선함으로써 유동성 조절의 효율성을 제고하였다.\n",
      "문서 내용: 입력: 23 Ⅰ 경제 동향 2 금융･외환시장 QR BOX QR1 통화신용정책보고서 웹페이지 QR2 경제전망보고서 웹페이지 QR3 금융안정보고서 웹페이지 QR4 지역경제보고서 웹페이지 QR5 한국은행 블로그  \n",
      "출력:주요 이슈.\n"
     ]
    }
   ],
   "source": [
    "query = '한국은행의 통화정책과 금융안정 정책의 차이점은 무엇인가요?'\n",
    "\n",
    "results = database.similarity_search(query, k=5) \n",
    "\n",
    "# 답변에서 중복 제거\n",
    "unique_results = {result.page_content: result for result in results}.values()\n",
    "\n",
    "print(f'문서 개수: {len(unique_results)}')\n",
    "\n",
    "for d in unique_results:\n",
    "    print(f'문서 내용: {d.page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 개수: 5\n",
      "문서 내용: 통화신용정책 한국은행은 통화신용정책의 효율적인 수립과 집행을 통해 물가 안정을 도모하고 금융 안정에도 유의하면서 국민경제의 건전한 발전에 이바지하기 위해 최선의 노력을 기울이고 있다. 통화정책 운영체계로 물가 안정 목표제를 채택하고 있으며 동 체계하에서도 정책 결정은 물가 상황 외에 경제 성장, 금융 안정 등을 종합적으로 고려하여 이루어진다.\n",
      "문서 내용: 기준금리를 3.5%의 긴축적인 수준으로 유지하고, 새마을금고 예금인출사태 등 금융·외환시장 불안에는 시장안정화 조치를 통해 적극 대처하였다. 한국은행은 경제상황에 대한 정확한 진단과 정교한 정책대응을 뒷받침하고자 조사 및 정책연구업무를 적극적으로 수행하였다.\n",
      "문서 내용: 한국은행은 이러한 정책 목표를 효과적으로 달성하는 방향으로 기준금리를 운영하고 있으며, 공개시장 운영, 여수신 제도, 지급준비 제도 등의 정책 수단을 활용하고 있다.\n",
      "문서 내용: 한국은행은 통화신용정책 등을 수행함에 있어 금융기관 등을 대상으로 여수신 업무를 수행한다. 2023년 중 운용한 여신 업무로는 금융중개지원대출, 자금조정대출, 일중당좌대출 등이 있으며, 수신 업무로는 당좌예금, 자금조정예금 및 통화 안정계정 예치금 등이 있다.\n",
      "문서 내용: 한국은행은 콜금리(익일물)를 금융통화위원회가 정한 기준금리 수준에서 유지하기 위해 통화안정증권, 환매조건부증권(RP), 통화안정계정 등 다양한 공개시장 운영 수단을 활용하여 유동성을 신축적으로 조절하였다. 아울러 공개시장 운영 여건 변화에 맞춰 통화안정증권 발행 제도를 개선함으로써 유동성 조절의 효율성을 제고하였다.\n"
     ]
    }
   ],
   "source": [
    "query = '한국은행의 최근 금융시장 안정화 조치는 무엇이었나요?'\n",
    "\n",
    "results = database.similarity_search(query, k=5) \n",
    "\n",
    "# 답변에서 중복 제거\n",
    "unique_results = {result.page_content: result for result in results}.values()\n",
    "\n",
    "print(f'문서 개수: {len(unique_results)}')\n",
    "\n",
    "for d in unique_results:\n",
    "    print(f'문서 내용: {d.page_content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "# MPS 디바이스 설정\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "#모델 설정\n",
    "chat = ChatOpenAI(model='gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'올해 한국은행의 정책목표 중 가장 중요한 것은 물가 안정입니다. 한국은행은 통화신용정책의 효율적인 수립과 집행을 통해 물가 안정을 도모하고, 금융 안정에도 유의하면서 국민경제의 건전한 발전을 위해 노력하고 있습니다.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Retriever 설정\n",
    "retriever = database.as_retriever()  # 데이터베이스를 Retriever로 변환\n",
    "\n",
    "get_answer = RetrievalQA.from_llm( #RetrievalQA를 초기화\n",
    "    llm=chat, #Chat models를 지정\n",
    "    retriever = retriever, #Retriever를 지정\n",
    "    return_source_documents=False #응답에 원본 문서를 포함할지 결정\n",
    ")\n",
    "\n",
    "query = \"올해 한국은행의 정책목표 중 가장 중요한 것은 무엇인가?\"\n",
    "\n",
    "def answer_to_query(query):\n",
    "    answer = get_answer(query)  # 답변 요청\n",
    "    result_text = answer.get('result', '')  # 결과에서 'result' 키를 안전하게 가져오기\n",
    "\n",
    "    # 특정 텍스트 패턴 제거 로직\n",
    "    if 'Helpful Answer:\\n' in result_text:\n",
    "        parts = result_text.split('Helpful Answer:\\n')\n",
    "        answer_parsed = parts[1].strip() if len(parts) > 1 else parts[0].strip()\n",
    "    elif 'Answer:' in result_text:\n",
    "        answer_parsed = result_text.split('Answer:')[-1].strip()\n",
    "    else:\n",
    "        answer_parsed = result_text.strip()  # 기본적으로 텍스트 전체를 반환\n",
    "\n",
    "    return answer_parsed\n",
    "\n",
    "answer_to_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'최근 한국은행이 기준금리를 조정한 이유는 대내외적인 정책 여건을 고려했기 때문입니다. 구체적으로는 물가 상승률이 둔화되었지만 여전히 목표를 상회하는 오름세가 이어지고 있었고, 경기와 금융안정 측면의 리스크가 존재하며, 가계부채 추이, 미국 연방준비제도의 통화정책, 지정학적 리스크 등의 높은 불확실성이 지속되고 있었기 때문입니다. 이러한 요인들을 고려하여 한국은행은 기준금리를 긴축적인 수준에서 유지하고, 기존의 3.0% 포인트 인상의 파급 효과를 점검하면서 추가 인상 필요성을 판단해 나갔습니다.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_to_query('최근 한국은행이 기준금리를 조정한 이유는 무엇인가요?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://b8faf96c94e08e7028.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b8faf96c94e08e7028.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /var/folders/hw/9m3g7fvn4_l3rp2y473km9sm0000gn/T/matplotlib-8lzqjkiu because the default path (/Users/jaesolshin/.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Retriever 설정 (database 및 huggingface_llm은 미리 정의되어 있어야 함)\n",
    "retriever = database.as_retriever()  # 데이터베이스를 Retriever로 변환\n",
    "\n",
    "# RetrievalQA 초기화\n",
    "get_answer = RetrievalQA.from_llm(\n",
    "    llm=chat,  # Chat models를 지정\n",
    "    retriever=retriever,  # Retriever를 지정\n",
    "    return_source_documents=False  # 응답에 원본 문서를 포함할지 결정\n",
    ")\n",
    "\n",
    "# 질문에 대한 답변 생성 함수 (RetrievalQA 활용)\n",
    "def chat_with_retrieval(message, history):\n",
    "    # 사용자의 질문에 대해 RetrievalQA를 사용하여 답변 생성\n",
    "    response = get_answer(message)\n",
    "    result_text = response.get('result', '')\n",
    "\n",
    "    # 리스트일 경우 이를 하나의 문자열로 변환\n",
    "    if isinstance(result_text, list):\n",
    "        result_text = ' '.join([str(item) for item in result_text])\n",
    "\n",
    "    # 특정 텍스트 패턴 제거 로직\n",
    "    if 'Helpful Answer:\\n' in result_text:\n",
    "        parts = result_text.split('Helpful Answer:\\n')\n",
    "        answer_parsed = parts[1].strip() if len(parts) > 1 else parts[0].strip()\n",
    "    elif 'Answer:' in result_text:\n",
    "        answer_parsed = result_text.split('Answer:')[-1].strip()\n",
    "    else:\n",
    "        answer_parsed = result_text.strip()\n",
    "\n",
    "\n",
    "    # 불필요한 패턴 (공백 + 문자 + :) 제거. 해당 패턴 이후 내용 제거\n",
    "    # 정규 표현식으로 공백 뒤에 [문자]: 형태를 탐색하여 그 뒤를 제거\n",
    "    answer_parsed  = re.split(r\"\\s\\w+:\", answer_parsed)[0].strip()\n",
    "    \n",
    "    # 질문과 답변을 히스토리에 저장 (history는 대화 히스토리)\n",
    "    history.append((message, answer_parsed))  \n",
    "    \n",
    "    # Gradio가 (응답, history)를 반환해야 하므로, 대화 기록과 함께 반환\n",
    "    return history, history\n",
    "\n",
    "# Gradio Chatbot 인터페이스 생성\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()  # 대화 기록을 표시하는 컴포넌트\n",
    "    msg = gr.Textbox(label=\"질문 입력\")  # 질문 입력을 위한 텍스트 박스\n",
    "    clear = gr.Button(\"대화 기록 초기화\")  # 대화 기록 초기화 버튼\n",
    "\n",
    "    # 대화가 시작될 때 실행할 동작 정의\n",
    "    msg.submit(chat_with_retrieval, inputs=[msg, chatbot], outputs=[chatbot, chatbot])\n",
    "\n",
    "    # 기록 초기화 버튼 동작 정의\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "# 앱 실행\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n\u001b[1;32m      8\u001b[0m     context \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m-----------------------------------\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mDoc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mpage_content\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 14\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mPromptTemplate\u001b[49m(\n\u001b[1;32m     15\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m경제전문가로서 다음 주어진 정보를 바탕으로 질문에 응답하시오. 1) 하나의 완결된 문장으로 작성. 중간에 끊기지 않도록 문장을 완성할 것. 2) 필요한 정보를 모두 담도록 할 것. 3) 출처를 명확히 제시할 것. 4) 개조식으로 작성하지 말 것\u001b[39m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m***Information***\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;132;01m{document}\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m***Question***\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;132;01m{query}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;124m***Answer***\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat(document\u001b[38;5;241m=\u001b[39mcontext, query\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chains import PromptTemplate\n",
    "\n",
    "query = \"2023년 통화정책의 가장 중요한 목표와 고려사항은?\"\n",
    "\n",
    "results = database.similarity_search(query, k=5) \n",
    "\n",
    "context = \"Relevant documents:\\n\"  #조회를 통해 얻은 정보를 저장할 변수 초기화\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    context += f\"\"\"\n",
    "-----------------------------------\n",
    "Doc {i+1}:\n",
    "{result.page_content}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"경제전문가로서 다음 주어진 정보를 바탕으로 질문에 응답하시오. 1) 하나의 완결된 문장으로 작성. 중간에 끊기지 않도록 문장을 완성할 것. 2) 필요한 정보를 모두 담도록 할 것. 3) 출처를 명확히 제시할 것. 4) 개조식으로 작성하지 말 것\n",
    "\n",
    "***Information***\n",
    "{document}\n",
    "\n",
    "***Question***\n",
    "{query} \n",
    "\n",
    "\n",
    "***Answer***\"\"\",\n",
    "    input_variables=['document', 'query']\n",
    ")\n",
    "\n",
    "prompt = prompt.format(document=context, query=query)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://1da2fea5d23b6a01b6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1da2fea5d23b6a01b6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import re\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Retriever 설정 (database 및 huggingface_llm은 미리 정의되어 있어야 함)\n",
    "retriever = database.as_retriever()  # 데이터베이스를 Retriever로 변환\n",
    "\n",
    "# 프롬프트 템플릿 설정\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"경제전문가로서 다음 주어진 정보를 바탕으로 질문에 응답하시오. 1) 144자 정도의 완결된 문장으로 작성할 것 2) 개조식으로 작성하지 말 것.\n",
    "\n",
    "***Information***\n",
    "{documents}\n",
    "\n",
    "***Question***\n",
    "{query}\n",
    "\n",
    "***Answer***\"\"\",\n",
    "    input_variables=['documents', 'query']\n",
    ")\n",
    "\n",
    "# RetrievalQA 초기화\n",
    "get_answer = RetrievalQA.from_llm(\n",
    "    llm=chat,  # Hugging Face LLM 모델 지정\n",
    "    retriever=retriever,  # Retriever를 지정\n",
    "    return_source_documents=True  # 응답에 원본 문서를 포함할지 여부\n",
    ")\n",
    "\n",
    "# 질문에 대한 답변 생성 함수 (RetrievalQA 활용)\n",
    "def chat_with_retrieval(message, history):\n",
    "    # 사용자의 질문에 대해 RetrievalQA를 사용하여 문서 검색 및 응답 생성\n",
    "    response = get_answer(message)\n",
    "    documents = \"\\n\".join([doc.page_content for doc in response.get('source_documents', [])])\n",
    "    \n",
    "    # 프롬프트 템플릿 적용\n",
    "    prompt = prompt_template.format(documents=documents, query=message)\n",
    "    \n",
    "    # LLM을 통해 답변 생성\n",
    "    llm_response = chat.predict(prompt)  # 여기서는 LLM 호출 결과가 문자열로 반환됩니다.\n",
    "    result_text = llm_response  # 이미 문자열이므로 `get` 대신 바로 result_text로 사용\n",
    "\n",
    "    # 불필요한 패턴 제거 로직\n",
    "    if '***Answer***' in result_text: \n",
    "        answer_parsed = result_text.split('***Answer***')[-1].strip()\n",
    "    elif '***information***' in result_text:\n",
    "        answer_parsed = result_text.split('***Question***')[-1].strip()\n",
    "    elif '***Answer***' in result_text:\n",
    "        answer_parsed = result_text.split('***Answer***')[-1].strip()        \n",
    "    else:\n",
    "        answer_parsed = result_text.strip()\n",
    "\n",
    "    # 불필요한 패턴 (공백 + 문자 + :) 제거\n",
    "    answer_parsed = re.sub(r'\\s\\w+:', '', answer_parsed).strip()\n",
    "\n",
    "    # 질문과 답변을 히스토리에 저장 (history는 대화 히스토리)\n",
    "    history.append((message, answer_parsed))\n",
    "\n",
    "    # Gradio가 (응답, history)를 반환해야 하므로, 대화 기록과 함께 반환\n",
    "    return history, history\n",
    "\n",
    "# Gradio Chatbot 인터페이스 생성\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()  # 대화 기록을 표시하는 컴포넌트\n",
    "    msg = gr.Textbox(label=\"질문 입력\")  # 질문 입력을 위한 텍스트 박스\n",
    "    clear = gr.Button(\"대화 기록 초기화\")  # 대화 기록 초기화 버튼\n",
    "\n",
    "    # 대화가 시작될 때 실행할 동작 정의\n",
    "    msg.submit(chat_with_retrieval, inputs=[msg, chatbot], outputs=[chatbot, chatbot])\n",
    "\n",
    "    # 기록 초기화 버튼 동작 정의\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "# 앱 실행\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
