{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/jaesolshin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#Use 3.12.4\n",
    "import os\n",
    "import markdown\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from huggingface_hub import login\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from langchain import FAISS\n",
    "from langchain.text_splitter import SpacyTextSplitter \n",
    "from langchain.document_loaders import PyMuPDFLoader, DirectoryLoader, TextLoader\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.docstore.document import Document\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "key_path = '/Users/jaesolshin/key/HF_TOKEN.txt'\n",
    "os.environ[\"HF_TOKEN\"] = open(key_path, 'r', encoding='utf-8').read()\n",
    "login(os.environ[\"HF_TOKEN\"], add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymupdf\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "path = '/Users/jaesolshin/Library/CloudStorage/GoogleDrive-ysys143@gmail.com/내 드라이브/2024-2/Google ML Bootcamp2024/data/bokbot/annual_report/2023년도 연차보고서.pdf'\n",
    "loader = PyMuPDFLoader(path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서개수: 191\n",
      "첫번째 문서의 내용: ISSN 1975-4922\n",
      "2023\n",
      "연차보고서\n",
      "2024. 3\n",
      "\n",
      "첫번째 문서의 메타데이터: {'source': '/Users/jaesolshin/Library/CloudStorage/GoogleDrive-ysys143@gmail.com/내 드라이브/2024-2/Google ML Bootcamp2024/data/bokbot/annual_report/2023년도 연차보고서.pdf', 'file_path': '/Users/jaesolshin/Library/CloudStorage/GoogleDrive-ysys143@gmail.com/내 드라이브/2024-2/Google ML Bootcamp2024/data/bokbot/annual_report/2023년도 연차보고서.pdf', 'page': 0, 'total_pages': 191, 'format': 'PDF 1.6', 'title': '', 'author': '1120077', 'subject': '', 'keywords': '', 'creator': 'Hwp 2018 10.0.0.10640', 'producer': 'Hancom PDF 1.3.0.538', 'creationDate': \"D:20240328151525+09'00'\", 'modDate': \"D:20240619161741+09'00'\", 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "print(f'문서개수: {len(documents)}')\n",
    "print(f'첫번째 문서의 내용: {documents[0].page_content}')\n",
    "print(f'첫번째 문서의 메타데이터: {documents[0].metadata}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 318, which is longer than the specified 300\n",
      "Created a chunk of size 434, which is longer than the specified 300\n",
      "Created a chunk of size 881, which is longer than the specified 300\n",
      "Created a chunk of size 342, which is longer than the specified 300\n",
      "Created a chunk of size 412, which is longer than the specified 300\n",
      "Created a chunk of size 323, which is longer than the specified 300\n",
      "Created a chunk of size 367, which is longer than the specified 300\n",
      "Created a chunk of size 312, which is longer than the specified 300\n",
      "Created a chunk of size 421, which is longer than the specified 300\n",
      "Created a chunk of size 494, which is longer than the specified 300\n",
      "Created a chunk of size 350, which is longer than the specified 300\n",
      "Created a chunk of size 744, which is longer than the specified 300\n",
      "Created a chunk of size 555, which is longer than the specified 300\n",
      "Created a chunk of size 453, which is longer than the specified 300\n",
      "Created a chunk of size 540, which is longer than the specified 300\n",
      "Created a chunk of size 302, which is longer than the specified 300\n",
      "Created a chunk of size 444, which is longer than the specified 300\n",
      "Created a chunk of size 433, which is longer than the specified 300\n",
      "Created a chunk of size 410, which is longer than the specified 300\n",
      "Created a chunk of size 325, which is longer than the specified 300\n",
      "Created a chunk of size 344, which is longer than the specified 300\n",
      "Created a chunk of size 1215, which is longer than the specified 300\n",
      "Created a chunk of size 681, which is longer than the specified 300\n",
      "Created a chunk of size 348, which is longer than the specified 300\n",
      "Created a chunk of size 414, which is longer than the specified 300\n",
      "Created a chunk of size 323, which is longer than the specified 300\n",
      "Created a chunk of size 320, which is longer than the specified 300\n",
      "Created a chunk of size 529, which is longer than the specified 300\n",
      "Created a chunk of size 438, which is longer than the specified 300\n",
      "Created a chunk of size 301, which is longer than the specified 300\n",
      "Created a chunk of size 308, which is longer than the specified 300\n",
      "Created a chunk of size 369, which is longer than the specified 300\n",
      "Created a chunk of size 445, which is longer than the specified 300\n",
      "Created a chunk of size 329, which is longer than the specified 300\n",
      "Created a chunk of size 610, which is longer than the specified 300\n",
      "Created a chunk of size 363, which is longer than the specified 300\n",
      "Created a chunk of size 1186, which is longer than the specified 300\n",
      "Created a chunk of size 423, which is longer than the specified 300\n",
      "Created a chunk of size 772, which is longer than the specified 300\n",
      "Created a chunk of size 841, which is longer than the specified 300\n",
      "Created a chunk of size 646, which is longer than the specified 300\n",
      "Created a chunk of size 389, which is longer than the specified 300\n",
      "Created a chunk of size 304, which is longer than the specified 300\n",
      "Created a chunk of size 337, which is longer than the specified 300\n",
      "Created a chunk of size 350, which is longer than the specified 300\n",
      "Created a chunk of size 1105, which is longer than the specified 300\n",
      "Created a chunk of size 1298, which is longer than the specified 300\n",
      "Created a chunk of size 1267, which is longer than the specified 300\n",
      "Created a chunk of size 470, which is longer than the specified 300\n",
      "Created a chunk of size 611, which is longer than the specified 300\n",
      "Created a chunk of size 310, which is longer than the specified 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할 전 문서 개수: 191\n",
      "분할 후 문서 개수: 826\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter #자연어 처리를 위한 여러 기능을 제공한다.\n",
    "text_splitter = SpacyTextSplitter(\n",
    "    chunk_size=300, #분할할 크기를 설정\n",
    "    chunk_overlap=50,\n",
    "    pipeline='ko_core_news_sm' #분할에 사용할 언어모델을 설정\n",
    ")\n",
    "\n",
    "splitted_documents = text_splitter.split_documents(documents) #문서를 분할\n",
    "\n",
    "print(f'분할 전 문서 개수: {len(documents)}')\n",
    "print(f'분할 후 문서 개수: {len(splitted_documents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hw/9m3g7fvn4_l3rp2y473km9sm0000gn/T/ipykernel_1740/1829115815.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents added to the FAISS index.\n"
     ]
    }
   ],
   "source": [
    "# SentenceTransformer 모델 로드\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# SentenceTransformer 적재를 위해 Document 객체에서 텍스트 추출\n",
    "texts = [doc.page_content for doc in splitted_documents]\n",
    "\n",
    "# 문서 임베딩 생성\n",
    "embeddings = embedding_model.embed_documents(texts)\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "d = len(embeddings[0])  # 임베딩 차원 (예: 384차원)\n",
    "index = faiss.IndexFlatL2(d)  # L2 거리 기반의 FAISS 인덱스 생성\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# LangChain의 FAISS와 연결\n",
    "docstore = InMemoryDocstore({i: Document(page_content=texts[i]) for i in range(len(texts))})\n",
    "faiss_index = FAISS(embedding_function=embedding_model, index=index, docstore=docstore, index_to_docstore_id={i: i for i in range(len(texts))})\n",
    "\n",
    "# FAISS 인덱스가 생성되었습니다.\n",
    "print(\"Documents added to the FAISS index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the given information.\n",
      "\n",
      "Information:\n",
      "Relevant documents:\n",
      "\n",
      "-----------------------------------\n",
      "Doc 1:\n",
      "한국은행 통합별관 준공 및 재입주 기념식\n",
      "(2023년 4월 24일)\n",
      "자료: 한국은행\n",
      "\n",
      "-----------------------------------\n",
      "Doc 2:\n",
      "화폐사랑 콘텐츠 공모전 시상식 \n",
      "              (2023년 11월 21일)\n",
      "자료: 한국은행\n",
      "\n",
      "-----------------------------------\n",
      "Doc 3:\n",
      "2023년말 현재 한국은행, \n",
      "금융기관, 유관기관 등 총 32개 기관이 참여하고 있으며, 산하에 금융정보화 공동추진사업의 선정, 금융표준화 등의 사전 심\n",
      "의를 위한 실무협의회를 두고 있다.\n",
      "\n",
      "-----------------------------------\n",
      "Doc 4:\n",
      "기업의전반적인채무상환능력은\n",
      "전기전자, 석유화학, 건설등업종의업황부진과\n",
      "금리상승등의영향으로저하되면서기업대출\n",
      "연체율이상승72)하였다.\n",
      "\n",
      "-----------------------------------\n",
      "Doc 5:\n",
      "먼저 일부 위원은 금융안정 책무수행에 \n",
      "있어 당행의 정책권한과 책임을 정립해 \n",
      "나갈 필요가 있으며, 이를 바탕으로 당\n",
      "\n",
      "\n",
      "Question:2023년 통화정책의 가장 중요한 목표와 고려사항은? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedding_model\n",
    "\n",
    "database = faiss_index\n",
    "\n",
    "query = \"2023년 통화정책의 가장 중요한 목표와 고려사항은?\"\n",
    "\n",
    "results = faiss_index.similarity_search(query, k=5) \n",
    "\n",
    "context = \"Relevant documents:\\n\"  #조회를 통해 얻은 정보를 저장할 변수 초기화\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    context += f\"\"\"\n",
    "-----------------------------------\n",
    "Doc {i+1}:\n",
    "{result.page_content}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Answer the question based on the given information.\n",
    "\n",
    "Information:\n",
    "{document}\n",
    "\n",
    "Question:{query} \n",
    "\"\"\",\n",
    "    input_variables=['documents', 'query']\n",
    ")\n",
    "\n",
    "prompt = prompt.format(document=context, query=query)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e826eb7020c84e28ad285edc43521212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "\n",
    "model_id = \"beomi/gemma-ko-2b\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/gemma-ko-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"mps\",\n",
    "    torch_dtype=dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the given information.\n",
      "\n",
      "Information:\n",
      "Relevant documents:\n",
      "\n",
      "-----------------------------------\n",
      "Doc 1:\n",
      "한국은행 통합별관 준공 및 재입주 기념식\n",
      "(2023년 4월 24일)\n",
      "자료: 한국은행\n",
      "\n",
      "-----------------------------------\n",
      "Doc 2:\n",
      "화폐사랑 콘텐츠 공모전 시상식 \n",
      "              (2023년 11월 21일)\n",
      "자료: 한국은행\n",
      "\n",
      "-----------------------------------\n",
      "Doc 3:\n",
      "2023년말 현재 한국은행, \n",
      "금융기관, 유관기관 등 총 32개 기관이 참여하고 있으며, 산하에 금융정보화 공동추진사업의 선정, 금융표준화 등의 사전 심\n",
      "의를 위한 실무협의회를 두고 있다.\n",
      "\n",
      "-----------------------------------\n",
      "Doc 4:\n",
      "기업의전반적인채무상환능력은\n",
      "전기전자, 석유화학, 건설등업종의업황부진과\n",
      "금리상승등의영향으로저하되면서기업대출\n",
      "연체율이상승72)하였다.\n",
      "\n",
      "-----------------------------------\n",
      "Doc 5:\n",
      "먼저 일부 위원은 금융안정 책무수행에 \n",
      "있어 당행의 정책권한과 책임을 정립해 \n",
      "나갈 필요가 있으며, 이를 바탕으로 당\n",
      "\n",
      "\n",
      "Question:2023년 통화정책의 가장 중요한 목표와 고려사항은? \n",
      "(1) 통화정책의 목표는 \n",
      "(2) 통화정책의 고려사항은 \n",
      "-----------------------------------\n",
      "Doc 1:\n",
      "한국은행 통합별관 준공 및 재입주 기념식\n",
      "(2023년 4월 24일)\n",
      "자료: 한국은행\n",
      "\n",
      "-----------------------------------\n",
      "Doc 2:\n",
      "화폐사랑 콘텐츠 공모전 시상식 \n",
      "              (2023년 11월 21일)\n",
      "자료: 한국은행\n",
      "\n",
      "-----------------------------------\n",
      "Doc 3:\n",
      "2023년말 현재 한국은행, \n",
      "금융기관, 유\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
