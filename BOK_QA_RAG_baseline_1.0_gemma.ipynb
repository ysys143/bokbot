{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A\n",
    "jeonseonjin/embedding_BAAI-bge-m3<br>\n",
    "<br>\n",
    "## RAG\n",
    "rectified_text_2023_2.txt<br>\n",
    "RecursiveCharacterTextSplitter<br>\n",
    "jeonseonjin/embedding_BAAI-bge-m3<br>\n",
    "FAISS<br>\n",
    "beomi/gemma-ko-2b<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myenv(3.12.6)\n",
    "#%pip install huggingface_hub langchain langchain-community faiss-cpu sentence-transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/jaesolshin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#Use 3.12.4\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "key_path = '/Users/jaesolshin/key/HF_TOKEN.txt'\n",
    "os.environ[\"HF_TOKEN\"] = open(key_path, 'r', encoding='utf-8').read()\n",
    "login(os.environ[\"HF_TOKEN\"], add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query-Answer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전정의된 Query-Answer가 담긴 테이블\n",
    "qna_df = pd.read_csv('qa_data.csv')[['질문', '답변']]\n",
    "\n",
    "qna_df['질문'] = qna_df['질문'].apply(lambda x: x.split('질문\\n')[1]) # \"질문\\n\" 제거\n",
    "qna_df['답변'] = qna_df['답변'].apply(lambda x: x.split('답변\\n')[1]) # \"답변\\n\" 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# SentenceTransformer 모델 로드\n",
    "embedding_model = SentenceTransformer('jeonseonjin/embedding_BAAI-bge-m3')\n",
    "\n",
    "# 쿼리 문장들에 대한 임베딩 벡터 생성\n",
    "query_texts = qna_df['질문'].to_list()\n",
    "query_embeddings = embedding_model.encode(query_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# query-answer 함수 정의\n",
    "def qna_answer_to_query(new_query, embedding_model=embedding_model, query_embeddings=query_embeddings, top_k=1, verbose=True):\n",
    "    # 쿼리 임베딩 계산\n",
    "    new_query_embedding = embedding_model.encode([new_query])\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    cos_sim = cosine_similarity(new_query_embedding, query_embeddings)\n",
    "    \n",
    "    # 코사인 유사도 값이 가장 큰 질문의 인덱스 찾기\n",
    "    most_similar_idx = np.argmax(cos_sim)\n",
    "    similarity = np.round(cos_sim[0][most_similar_idx], 2)\n",
    "    \n",
    "    # 가장 비슷한 질문과 답변 가져오기\n",
    "    similar_query = query_texts[most_similar_idx]\n",
    "    similar_answer = qna_df.iloc[most_similar_idx]['답변']\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(\"가장 비슷한 질문 : \", similar_query)\n",
    "        print(\"가장 비슷한 질문의 유사도 : \", similarity)\n",
    "        print(\"가장 비슷한 질문의 답: \", similar_answer)\n",
    "\n",
    "    # 결과 반환\n",
    "    return similar_query, similarity, similar_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국은행은 1962년 이후에 발행된 원 표시 화폐에 대해서만 액면 금액으로 교환해 드리고 있습니다. 기타 옛날 돈은 한국은행에서 현용화폐로 교환할 수 없으며, 이러한 화폐는 화폐수집가들의 소장용으로 거래되기도 하므로 가까운 화폐수집상에 문의하시기 바랍니다. '"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 함수 호출 예시\n",
    "new_query = '옛날 돈의 가치'\n",
    "\n",
    "qna_answer_to_query(new_query, verbose=False)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loaders\n",
    "path = '/Users/jaesolshin/Documents/GitHub/bokbot/rectified_text_2023.txt'\n",
    "loader = TextLoader(path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할 전 문서 개수: 1\n",
      "분할 후 문서 개수: 61\n"
     ]
    }
   ],
   "source": [
    "# Text Splitting\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# splitter 정의\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \"]\n",
    ")\n",
    "\n",
    "# Document 형식의 보고서를 spliiter로 분할\n",
    "splitted_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# 문장이 마침표로 시작하면 마침표 제거, 마침표로 끝나지 않으면 마침표 추가\n",
    "for i in range(len(splitted_documents)):\n",
    "    splitted_documents[i].page_content = splitted_documents[i].page_content.strip('. ').strip()\n",
    "    if not splitted_documents[i].page_content.endswith('.'):\n",
    "        splitted_documents[i].page_content += '.'\n",
    "\n",
    "# 분할 결과 출력\n",
    "print(f'분할 전 문서 개수: {len(documents)}')\n",
    "print(f'분할 후 문서 개수: {len(splitted_documents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#분할된 문장을 벡터db에 저장\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.docstore.document import Document\n",
    "import faiss\n",
    "\n",
    "# SentenceTransformer 모델 로드\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"jeonseonjin/embedding_BAAI-bge-m3\")\n",
    "\n",
    "# SentenceTransformer 적재를 위해 Document 객체에서 텍스트 추출\n",
    "report_texts = [doc.page_content for doc in splitted_documents]\n",
    "\n",
    "# 문서 임베딩 생성\n",
    "embeddings = embedding_model.embed_documents(report_texts)\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "dim = len(embeddings[0])  # 임베딩 차원\n",
    "#report_index = faiss.IndexFlatIP(dim)  # 내적 기반 인덱스 생성\n",
    "report_index = faiss.IndexFlatL2(dim)  # L2 거리 기반의 인덱스 생성\n",
    "report_index.add(np.array(embeddings))\n",
    "\n",
    "# FAISS 인덱스를 파일로 저장\n",
    "faiss.write_index(report_index, 'report_index.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index loaded from file.\n"
     ]
    }
   ],
   "source": [
    "# Retrieval\n",
    "\n",
    "# 벡터db에 연결\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.docstore.document import Document\n",
    "from langchain import FAISS\n",
    "\n",
    "# FAISS 인덱스를 파일에서 불러오기\n",
    "if os.path.exists('faiss_index.bin'):\n",
    "    index = faiss.read_index('faiss_index.bin')\n",
    "    print(\"FAISS index loaded from file.\")\n",
    "else:\n",
    "    print(\"FAISS index file not found.\")\n",
    "\n",
    "# SentenceTransformer 모델 로드\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"jeonseonjin/embedding_BAAI-bge-m3\")\n",
    "\n",
    "# LangChain의 FAISS와 연결\n",
    "docstore = InMemoryDocstore({idx: Document(page_content=text) for idx, text in enumerate(report_texts)})\n",
    "docstore_id_map = {idx: idx for idx in range(len(report_texts))}\n",
    "database = FAISS(embedding_function=embedding_model, index=index, docstore=docstore, index_to_docstore_id=docstore_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 개수: 5\n",
      "문서 내용: 한국은행은 이러한 정책 목표를 효과적으로 달성하는 방향으로 기준금리를 운영하고 있으며, 공개시장 운영, 여수신 제도, 지급준비 제도 등의 정책 수단을 활용하고 있다.\n",
      "문서 내용: 한국은행은 이러한 대내외 정책 여건, 즉 물가 상승률이 기조적인 둔화 흐름을 이어갔지만 연중 물가 목표를 상회하는 오름세가 이어지고 경기와 금융안정 측면의 리스크가 상존하고 있었던 데다가 가계부채 추이, 미연준 통화정책, 지정학적 리스크 등과 관련한 높은 불확실성도 지속된 점을 고려할 때 기준금리를 긴축적인 수준에서 유지하고 그간의 3.0% 포인트 인상의 파급 효과를 점검하면서 추가 인상 필요성을 판단해 나가는 것이 필요하다.주요 이슈이다.\n",
      "문서 내용: 한국은행은 콜금리(익일물)를 금융통화위원회가 정한 기준금리 수준에서 유지하기 위해 통화안정증권, 환매조건부증권(RP), 통화안정계정 등 다양한 공개시장 운영 수단을 활용하여 유동성을 신축적으로 조절하였다. 아울러 공개시장 운영 여건 변화에 맞춰 통화안정증권 발행 제도를 개선함으로써 유동성 조절의 효율성을 제고하였다.\n",
      "문서 내용: 기준금리를 3.5%의 긴축적인 수준으로 유지하고, 새마을금고 예금인출사태 등 금융·외환시장 불안에는 시장안정화 조치를 통해 적극 대처하였다. 한국은행은 경제상황에 대한 정확한 진단과 정교한 정책대응을 뒷받침하고자 조사 및 정책연구업무를 적극적으로 수행하였다.\n",
      "문서 내용: 나. 기준금리 한국은행은 2023년 중 성장세를 점검하면서 중기적 시계에서 물가 상승률이 목표 수준(2%)에서 안정될 수 있도록 기준금리를 긴축적인 수준에서 운용하였다.\n"
     ]
    }
   ],
   "source": [
    "query = '최근 한국은행이 기준금리를 조정한 이유는 무엇인가요?'\n",
    "\n",
    "results = database.similarity_search(query, k=5) \n",
    "\n",
    "# 답변에서 중복 제거\n",
    "unique_results = {result.page_content: result for result in results}.values()\n",
    "\n",
    "print(f'문서 개수: {len(unique_results)}')\n",
    "\n",
    "for d in unique_results:\n",
    "    print(f'문서 내용: {d.page_content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "# MPS 디바이스 설정\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "#모델 설정\n",
    "model_id = \"beomi/gemma-ko-2b\"\n",
    "dtype = torch.bfloat16 if torch.backends.mps.is_available() else torch.float32\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=dtype  # torch_dtype 설정\n",
    ")\n",
    "\n",
    "# 모델을 MPS로 이동\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face pipeline을 사용하여 LLM 구성\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,  \n",
    "    temperature=0.7, \n",
    "    top_k=50,  \n",
    "    top_p=0.9,\n",
    "    do_sample=True, \n",
    "    truncation=True,\n",
    "    device=0 if device.type == \"mps\" else -1  # MPS가 있으면 0, 없으면 CPU\n",
    ")\n",
    "\n",
    "# LangChain에서 Hugging Face 모델을 LLM으로 사용\n",
    "huggingface_llm = HuggingFacePipeline(pipeline=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Retriever 설정\n",
    "retriever = database.as_retriever()  # 데이터베이스를 Retriever로 변환\n",
    "\n",
    "get_answer = RetrievalQA.from_llm( #RetrievalQA를 초기화\n",
    "    llm=huggingface_llm, #Chat models를 지정\n",
    "    retriever = retriever, #Retriever를 지정\n",
    "    return_source_documents=False #응답에 원본 문서를 포함할지 결정\n",
    ")\n",
    "\n",
    "def rag_answer_to_query(query):\n",
    "    answer = get_answer(query)  # 답변 요청\n",
    "    result_text = answer.get('result', '')  # 결과에서 'result' 키를 안전하게 가져오기\n",
    "\n",
    "    # 특정 텍스트 패턴 제거 로직\n",
    "    if 'Helpful Answer:\\n' in result_text:\n",
    "        parts = result_text.split('Helpful Answer:\\n')\n",
    "        answer_parsed = parts[1].strip() if len(parts) > 1 else parts[0].strip()\n",
    "    elif 'Answer:' in result_text:\n",
    "        answer_parsed = result_text.split('Answer:')[-1].strip()\n",
    "    else:\n",
    "        answer_parsed = result_text.strip()  # 기본적으로 텍스트 전체를 반환\n",
    "\n",
    "    return answer_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023년 한국은행 통화정책 운영의 목표는 3.6%의 소비자물가 상승률을 달성하기 위해 물가 안정 목표제를 운영하면서 물가 상승률을 낮추고, 경제성장을 촉진하며, 금융 안정을 유지하는 것이다.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_query = \"2023년 한국은행 통화정책 운영의 목표는 무엇인가?\"\n",
    "\n",
    "rag_answer_to_query(new_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_answer_to_query(new_query, critical_value=0.3):\n",
    "\n",
    "    qna_result = qna_answer_to_query(new_query, verbose=False)\n",
    "    qna_similarity = qna_result[1]\n",
    "    qna_answer = qna_result[2]\n",
    "\n",
    "    if qna_similarity >= critical_value:\n",
    "        msg = 'Q&A에 등재된 내용을 기반으로 답을 구합니다.\\n\\n'\n",
    "        return msg + qna_answer\n",
    "    \n",
    "    else:\n",
    "        msg = 'Q&A에 등재된 질문이 없어 연차보고서를 기반으로 답을 구합니다.\\n\\n'\n",
    "        return msg + rag_answer_to_query(new_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Q&A에 등재된 내용을 기반으로 답을 구합니다.\\n\\n한국은행은 중앙은행으로서 '정부의 은행' 역할을 수행하고 있습니다. 즉, 한국은행은 ...\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_query = \"2023년 한국은행 통화정책 운영의 목표는 무엇인가?\"\n",
    "\n",
    "full_answer_to_query(new_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문에 대한 답변 생성 함수 (RetrievalQA 활용)\n",
    "def chat_with_full_answer_to_query(message, history):\n",
    "    # 사용자의 질문에 대해 full_answer_to_query를 사용하여 답변 생성\n",
    "    response = full_answer_to_query(message)\n",
    "    \n",
    "    # 질문과 답변을 히스토리에 저장 (history는 대화 히스토리)\n",
    "    history.append((message, response))  \n",
    "    \n",
    "    # Gradio가 (응답, history)를 반환해야 하므로, 대화 기록과 함께 반환\n",
    "    return history, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "Running on public URL: https://ed9f869630eafd84ca.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ed9f869630eafd84ca.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Gradio Chatbot 인터페이스 생성\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()  # 대화 기록을 표시하는 컴포넌트\n",
    "    msg = gr.Textbox(label=\"질문 입력\")  # 질문 입력을 위한 텍스트 박스\n",
    "    clear = gr.Button(\"대화 기록 초기화\")  # 대화 기록 초기화 버튼\n",
    "\n",
    "    # 대화가 시작될 때 실행할 동작 정의\n",
    "    def clear_history():\n",
    "        return []\n",
    "\n",
    "    msg.submit(chat_with_full_answer_to_query, inputs=[msg, chatbot], outputs=[chatbot, chatbot])\n",
    "\n",
    "    # 기록 초기화 버튼 동작 정의\n",
    "    clear.click(clear_history, None, chatbot, queue=False)\n",
    "\n",
    "# 앱 실행\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
