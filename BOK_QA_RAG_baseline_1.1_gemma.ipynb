{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A\n",
    "jeonseonjin/embedding_BAAI-bge-m3<br>\n",
    "<br>\n",
    "## RAG\n",
    "rectified_text_2023_2.txt<br>\n",
    "RecursiveCharacterTextSplitter<br>\n",
    "jeonseonjin/embedding_BAAI-bge-m3<br>\n",
    "FAISS<br>\n",
    "beomi/gemma-ko-2b<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myenv(3.12.6)\n",
    "#%pip install huggingface_hub langchain langchain-community faiss-cpu sentence-transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/jaesolshin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#Use 3.12.4\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "key_path = '/Users/jaesolshin/key/HF_TOKEN.txt'\n",
    "os.environ[\"HF_TOKEN\"] = open(key_path, 'r', encoding='utf-8').read()\n",
    "login(os.environ[\"HF_TOKEN\"], add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query-Answer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전정의된 Query-Answer가 담긴 테이블\n",
    "qna_df = pd.read_csv('qa_data.csv')[['질문', '답변']]\n",
    "\n",
    "qna_df['질문'] = qna_df['질문'].apply(lambda x: x.split('질문\\n')[1]) # \"질문\\n\" 제거\n",
    "qna_df['답변'] = qna_df['답변'].apply(lambda x: x.split('답변\\n')[1]) # \"답변\\n\" 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# SentenceTransformer 모델 로드\n",
    "embedding_model = SentenceTransformer('jeonseonjin/embedding_BAAI-bge-m3')\n",
    "\n",
    "# 쿼리 문장들에 대한 임베딩 벡터 생성\n",
    "query_texts = qna_df['질문'].to_list()\n",
    "query_embeddings = embedding_model.encode(query_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# query-answer 함수 정의\n",
    "def qna_answer_to_query(new_query, embedding_model=embedding_model, query_embeddings=query_embeddings, top_k=1, verbose=True):\n",
    "    # 쿼리 임베딩 계산\n",
    "    new_query_embedding = embedding_model.encode([new_query])\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    cos_sim = cosine_similarity(new_query_embedding, query_embeddings)\n",
    "    \n",
    "    # 코사인 유사도 값이 가장 큰 질문의 인덱스 찾기\n",
    "    most_similar_idx = np.argmax(cos_sim)\n",
    "    similarity = np.round(cos_sim[0][most_similar_idx], 2)\n",
    "    \n",
    "    # 가장 비슷한 질문과 답변 가져오기\n",
    "    similar_query = query_texts[most_similar_idx]\n",
    "    similar_answer = qna_df.iloc[most_similar_idx]['답변']\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(\"가장 비슷한 질문 : \", similar_query)\n",
    "        print(\"가장 비슷한 질문의 유사도 : \", similarity)\n",
    "        print(\"가장 비슷한 질문의 답: \", similar_answer)\n",
    "\n",
    "    # 결과 반환\n",
    "    return similar_query, similarity, similar_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loaders\n",
    "path = '/Users/jaesolshin/Documents/GitHub/bokbot/rectified_text_2023.txt'\n",
    "loader = TextLoader(path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitting\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# splitter 정의\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \"]\n",
    ")\n",
    "\n",
    "# Document 형식의 보고서를 spliiter로 분할\n",
    "splitted_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# 문장이 마침표로 시작하면 마침표 제거, 마침표로 끝나지 않으면 마침표 추가\n",
    "for i in range(len(splitted_documents)):\n",
    "    splitted_documents[i].page_content = splitted_documents[i].page_content.strip('. ').strip()\n",
    "    if not splitted_documents[i].page_content.endswith('.'):\n",
    "        splitted_documents[i].page_content += '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#분할된 문장을 벡터db에 저장\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.docstore.document import Document\n",
    "from langchain import FAISS\n",
    "import faiss\n",
    "\n",
    "# SentenceTransformer 모델 로드\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"jeonseonjin/embedding_BAAI-bge-m3\")\n",
    "\n",
    "# SentenceTransformer 적재를 위해 Document 객체에서 텍스트 추출\n",
    "report_texts = [doc.page_content for doc in splitted_documents]\n",
    "\n",
    "# 문서 임베딩 생성\n",
    "embeddings = embedding_model.embed_documents(report_texts)\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "dim = len(embeddings[0])  # 임베딩 차원\n",
    "#report_index = faiss.IndexFlatIP(dim)  # 내적 기반 인덱스 생성\n",
    "report_index = faiss.IndexFlatL2(dim)  # L2 거리 기반의 인덱스 생성\n",
    "report_index.add(np.array(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain의 FAISS와 연결\n",
    "docstore = InMemoryDocstore({idx: Document(page_content=text) for idx, text in enumerate(report_texts)})\n",
    "docstore_id_map = {idx: idx for idx in range(len(report_texts))}\n",
    "database = FAISS(embedding_function=embedding_model, index=report_index, docstore=docstore, index_to_docstore_id=docstore_id_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "# MPS 디바이스 설정\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "#모델 설정\n",
    "model_id = \"beomi/gemma-ko-2b\"\n",
    "dtype = torch.bfloat16 if torch.backends.mps.is_available() else torch.float32\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=dtype  # torch_dtype 설정\n",
    ")\n",
    "\n",
    "# 모델을 MPS로 이동\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face pipeline을 사용하여 LLM 구성\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,  \n",
    "    temperature=0.7, \n",
    "    top_k=50,  \n",
    "    top_p=0.9,\n",
    "    do_sample=True, \n",
    "    truncation=True,\n",
    "    device=0 if device.type == \"mps\" else -1  # MPS가 있으면 0, 없으면 CPU\n",
    ")\n",
    "\n",
    "# LangChain에서 Hugging Face 모델을 LLM으로 사용\n",
    "huggingface_llm = HuggingFacePipeline(pipeline=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Retriever 설정\n",
    "retriever = database.as_retriever()  # 데이터베이스를 Retriever로 변환\n",
    "\n",
    "get_answer = RetrievalQA.from_llm( #RetrievalQA를 초기화\n",
    "    llm=huggingface_llm, #Chat models를 지정\n",
    "    retriever = retriever, #Retriever를 지정\n",
    "    return_source_documents=False #응답에 원본 문서를 포함할지 결정\n",
    ")\n",
    "\n",
    "def rag_answer_to_query(query):\n",
    "    answer = get_answer(query)  # 답변 요청\n",
    "    result_text = answer.get('result', '')  # 결과에서 'result' 키를 안전하게 가져오기\n",
    "\n",
    "    # 특정 텍스트 패턴 제거 로직\n",
    "    if 'Helpful Answer:\\n' in result_text:\n",
    "        parts = result_text.split('Helpful Answer:\\n')\n",
    "        answer_parsed = parts[1].strip() if len(parts) > 1 else parts[0].strip()\n",
    "    elif 'Answer:' in result_text:\n",
    "        answer_parsed = result_text.split('Answer:')[-1].strip()\n",
    "    elif 'Context:' in result_text:\n",
    "        answer_parsed = result_text.split('Context:')[0].strip()    \n",
    "    else:\n",
    "        answer_parsed = result_text.strip()  # 기본적으로 텍스트 전체를 반환\n",
    "\n",
    "    return answer_parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_answer_to_query(new_query, critical_value=0.3):\n",
    "\n",
    "    qna_result = qna_answer_to_query(new_query, verbose=False)\n",
    "    qna_similarity = qna_result[1]\n",
    "    qna_answer = qna_result[2]\n",
    "\n",
    "    if qna_similarity >= critical_value:\n",
    "        msg = 'Q&A에 등재된 내용을 기반으로 답을 구합니다.\\n\\n'\n",
    "        return msg + qna_answer\n",
    "    \n",
    "    else:\n",
    "        msg = 'Q&A에 등재된 질문이 없어 연차보고서를 기반으로 답을 구합니다.\\n\\n'\n",
    "        return msg + rag_answer_to_query(new_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문에 대한 답변 생성 함수 (RetrievalQA 활용)\n",
    "def chat_with_full_answer_to_query(message, history):\n",
    "    # 사용자의 질문에 대해 full_answer_to_query를 사용하여 답변 생성\n",
    "    response = full_answer_to_query(message)\n",
    "    \n",
    "    # 질문과 답변을 히스토리에 저장 (history는 대화 히스토리)\n",
    "    history.append((message, response))  \n",
    "    \n",
    "    # Gradio가 (응답, history)를 반환해야 하므로, 대화 기록과 함께 반환\n",
    "    return history, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "Running on public URL: https://2e8d0519aed3568df1.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2e8d0519aed3568df1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Gradio Chatbot 인터페이스 생성\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()  # 대화 기록을 표시하는 컴포넌트\n",
    "    msg = gr.Textbox(label=\"질문 입력\")  # 질문 입력을 위한 텍스트 박스\n",
    "    clear = gr.Button(\"대화 기록 초기화\")  # 대화 기록 초기화 버튼\n",
    "\n",
    "    # 대화가 시작될 때 실행할 동작 정의\n",
    "    def clear_history():\n",
    "        return []\n",
    "\n",
    "    msg.submit(chat_with_full_answer_to_query, inputs=[msg, chatbot], outputs=[chatbot, chatbot])\n",
    "\n",
    "    # 기록 초기화 버튼 동작 정의\n",
    "    clear.click(clear_history, None, chatbot, queue=False)\n",
    "\n",
    "# 앱 실행\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
